#!/usr/bin/env python3

import os
import argparse 
from pathlib import Path 
from dotenv import load_dotenv
import asyncio  # Import asyncio for running async functions

from autogen_core.models import UserMessage, SystemMessage
from autogen_ext.models.openai import AzureOpenAIChatCompletionClient
from azure.identity import DefaultAzureCredential

def find_dotenv_file():
    """Search for a .env file in current directory and parent directories"""
    current_dir = Path.cwd()
    while current_dir != current_dir.parent:  # Stop at root directory
        env_path = current_dir / '.env'
        if env_path.exists():
            return str(env_path)
        current_dir = current_dir.parent
    return None  # No .env file found

async def hypothesizer(user_input: str, research_document: str) -> str:
    """
    Hypothesizer agent that combines user input, a markdown document, and its own prompt
    to generate output using an OpenAI model on Azure.

    Args:
        user_input (str): A string provided by the user.
        research_document (str): A longer string containing a complete markdown document.

    Returns:
        str: The output generated by the LLM.
    """
    # Define the system prompt for the hypothesizer
    system_prompt = """
        You are an expert in cybersecurity threat hunting. As the hypothesis agent,
        your task is to generate hypotheses based on the user's input
        and the provided threat hunting behavior or technique research document.
        Use the following guidelines:
        1. Carefully analyze the user's input to understand the context, intent, 
           guidelines, or restrictions they may have.
        2. Extract relevant information from the research document.
        3. Combine both to generate clear, actionable, testable hypotheses.

        Respond with a list of potential hypotheses, along with any relevant 
        details or considerations, such as where on the network the hypothesis 
        could be tested (e.g., "on the endpoint", "on the network", 
        "on the SMTP servers", "Internet-facing services", etc.) and typical datasets or 
        types of data that would be necessary to test the hypothesis.

        Your output should be a Markdown document, with one section per hypothesis. 
        The title of the document should be "Suggested Hypotheses for Hunting (BEHAVIOR)",
        where "BEHAVIOR" is the behavior or technique being researched. (e.g., "Suggested
        Hypotheses for Hunting Credential Dumping"). That would be the first level header.

        Each section should include:
        - A title for the hypothesis. This should be the 2nd-level header (e.g., the 
          title of the section).
        - The actual hypothesis. Call this section "Hypothesis".
        - Test locations or systems. Call this section "Test Locations".
        - Datasets or types of data needed for testing. Call this section "Data".
        - Any other relevant details or considerations. Call this section 
          "Notes & Considerations".

        Do not include any other text or explanations outside of the sections
        and headers. The first line of the output should be the title of the document (the 
        1st level header) with no other text before it. When finished with the final 
        hypothesis section, end the document and do not include any summary or ask for 
        more work.
    
        The output should be a well-structured Markdown document.
        If you cannot generate a hypothesis, please say "No hypothesis could be generated."
    """

    messages = [
        SystemMessage(content=system_prompt),
        UserMessage(content=f"Here is the research document:\n{research_document}\n", source="user"),
        UserMessage(content=f"Here is the user's input: {user_input}\n", source="user")
    ]

    az_model_client = AzureOpenAIChatCompletionClient(
        azure_deployment=os.getenv("AZURE_OPENAI_DEPLOYMENT"),
        model=os.getenv("AZURE_OPENAI_MODEL"),
        api_version=os.getenv("AZURE_OPENAI_API_VERSION"),
        azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
        api_key=os.getenv("AZURE_OPENAI_API_KEY")
    )

    # Call the LLM using the AzureOpenAIChatCompletionClient
    try:
        result = await az_model_client.create(messages)  # Await the async method
        # Access the content from the CreateResult object
        return result.content  # Use the correct attribute to access the generated content
    except Exception as e:
        print(f"Error while generating hypotheses: {e}")
        return "An error occurred while generating hypotheses."

# Example usage
if __name__ == "__main__":

    # Set up argument parser
    parser = argparse.ArgumentParser(description='Given a threat hunting technique dossier, generate potential hypotheses for the hunter.')
    parser.add_argument('-e', '--environment', help='Path to specific .env file to use')
    parser.add_argument('-r', '--research', help='Path to the research document (markdown file)', required=True)
    parser.add_argument('-u', '--user_input', help='User input for hypothesis generation', required=False, default="")
    args = parser.parse_args()

    # Load environment variables
    if args.environment:
        # Use the specified .env file
        dotenv_path = args.environment
        if not os.path.exists(dotenv_path):
            print(f"Error: Specified environment file '{dotenv_path}' not found")
            exit(1)
        load_dotenv(dotenv_path)
    else:
        # Search for .env file
        dotenv_path = find_dotenv_file()
        if dotenv_path:
            load_dotenv(dotenv_path)
        else:
            print("Warning: No .env file found in current or parent directories")

    # Read the contents of the research document
    try:
        with open(args.research, 'r', encoding='utf-8') as file:
            research_data = file.read()
    except FileNotFoundError:
        print(f"Error: Research document '{args.research}' not found")
        exit(1)
    except Exception as e:
        print(f"Error reading research document: {e}")
        exit(1)

    # Run the hypothesizer asynchronously
    hypotheses = asyncio.run(hypothesizer(args.user_input, research_data))
    print(hypotheses)
