from autogen_core.models import UserMessage, SystemMessage

from utils.assistant_auth import PEAKAssistantAuthManager
from utils.azure_client import PEAKAssistantAzureOpenAIClient

from autogen_agentchat.base import TaskResult

from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import TextMentionTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.ui import Console
from autogen_agentchat.messages import TextMessage


async def hypothesizer(
    user_input: str, research_document: str, local_context: str
) -> str:
    """
    Hypothesizer agent that combines user input, a markdown document, and its own prompt
    to generate output using an OpenAI model on Azure.

    Args:
        user_input (str): A string provided by the user.
        research_document (str): A longer string containing a complete markdown document.

    Returns:
        str: The output generated by the LLM.
    """
    # Define the system prompt for the hypothesizer
    system_prompt = """
        You are an expert in cybersecurity threat hunting. As the hypothesis agent,
        your task is to generate hypotheses based on the user's input
        and the provided threat hunting behavior or technique research document.
        Use the following guidelines:
        1. Carefully analyze the user's input to understand the context, intent, 
           guidelines, or restrictions they may have.
        2. Extract relevant information from the research document.
        3. Combine both to generate clear, actionable, testable hypotheses.

        Respond with a list of potential hunting hypotheses.

        Your output should be a list of hypotheses, each on a line by itself. There
        should be no additional text, explanations, or formatting. Include at least
        three hypotheses, but feel free to generate more if you can.
        
        If you cannot generate a hypothesis, please say "No hypothesis could be generated."
    """

    messages = [
        SystemMessage(content=system_prompt),
        UserMessage(
            content=f"Here is the research document:\n{research_document}\n",
            source="user",
        ),
        UserMessage(content=f"Here is the user's input: {user_input}\n", source="user"),
        UserMessage(
            content=f"Additional local context: {local_context}\n", source="user"
        ),
    ]

    auth_mgr = PEAKAssistantAuthManager()
    az_model_client = await PEAKAssistantAzureOpenAIClient().get_client(
        auth_mgr=auth_mgr
    )

    # Call the LLM using the AzureOpenAIChatCompletionClient
    try:
        result = await az_model_client.create(messages)  # Await the async method
        # Access the content from the CreateResult object
        # Use the correct attribute to access the generated content
        return str(result.content)
    except Exception as e:
        print(f"Error while generating hypotheses: {e}")
        return "An error occurred while generating hypotheses."


async def refiner(
    hypothesis: str,
    local_context: str,
    research_document: str,
    verbose: bool = False,
    previous_run: list = list(),
) -> TaskResult:
    """
    Threat hunting hypothesis refiner agent that combines user input, a markdown document, and its own prompt
    to generate output using an OpenAI model on Azure.

    Args:
        hypothesis (str): A string provided by the user containing a threat hunting hypothesis.
        user_input (str): A string provided by the user containing additional context or information.
        research_document (str): A longer string containing a complete markdown document.

    Returns:
        str: The output generated by the LLM.
    """
    refiner_system_prompt = """
        You are an expert in cybersecurity threat hunting. Your job is to help the user improve
        their existing threat hunting hypothesis into its best form.

        You will be provided with the hypothesis, a research document, any additional guidelines
        or context provided by the user, and a list of up to 5 reasons why the hypothesis is not 
        a good and how it could be improved. Implement the suggested improvements and return the
        improved hypothesis. If you are unable to implement any of the suggestions, please explain why
        but return the (possibly improved) hypothesis.

        Sample good hypotheses:

        "Data exfiltration is occurring via the use of DNS tunneling techniques 
        whereby the exfiltrated data is encoded as part of the DNS subdomain."

        "Lateral movement is occurring via the use of legitimate but compromised 
        credentials and LOLBins."

        "An adversay may be using a webshell on an Internet-facing web server to execute
        commands on the server and exfiltrate data to a remote server."
    """

    critic_system_prompt = """
        You are an expert in cybersecurity threat hunting. Your job is to tell help the user refine 
        their existing threat hunting hypothesis into it's best form. 
        You will be provided with the initial hypothesis, a research document, and any additional 
        guidelines or context provided by the user.

        Your primary criteria for a "good" hypothesis is whether the given hypothesis is testable or not. 
        However, there may be other criteria that you may consider as well, such as whether the hypothesis 
        is specific enough, whether it is relevant to the research document, and whether it is actionable. 
        
        For each input hypothesis, if you believe it is a "good" hypothesis, you will return the hypothesis as-is, 
        followed by the string "YYY-HYPOTHESIS-ACCEPTED-YYY" on a new line. For "good" hypotheses, do not
        include any additional commentary or notes of your own. If the hypothesis is not a good
        hypothesis, you will return a list of up to 5 reasons why you believe it is not a good hypothesis and 
        what the user could do to correct these issues. Do not improve the hypothesis yourself.

        Assume the user has all legal, ethical, and moral authority and responsibility to perform any network, 
        host, or other monitoring necessary to test their hypothesis.
    """

    auth_mgr = PEAKAssistantAuthManager()
    az_model_client = await PEAKAssistantAzureOpenAIClient().get_client(
        auth_mgr=auth_mgr
    )
    # az_model_reasoning_client = await PEAKAssistantAzureOpenAIClient().get_client(
    #     auth_mgr=auth_mgr, model_type="reasoning"
    # )

    # Create the primary agent.
    refiner_agent = AssistantAgent(
        "refiner", model_client=az_model_client, system_message=refiner_system_prompt
    )

    # Create the critic agent.
    critic_agent = AssistantAgent(
        "critic", model_client=az_model_client, system_message=critic_system_prompt
    )

    # Define a termination condition that stops the task if the critic approves.
    text_termination = TextMentionTermination("YYY-HYPOTHESIS-ACCEPTED-YYY")

    # Create a team with the primary and critic agents.
    team = RoundRobinGroupChat(
        [critic_agent, refiner_agent], termination_condition=text_termination
    )

    # Always add these, no matter if it's the first run or a subsequent one
    messages = [
        TextMessage(
            content=f"Here is the user's hypothesis: {hypothesis}\n", source="user"
        ),
        TextMessage(
            content=f"Here is the research document:\n{research_document}\n",
            source="user",
        ),
        TextMessage(
            content=f"Additional local context: {local_context}\n", source="user"
        ),
    ]

    # If we have messages from a previous run, add them so we can continue the research
    if previous_run:
        messages = messages + previous_run

    try:
        # Run the team asynchronously
        if verbose:
            result = await Console(team.run_stream(task=messages), output_stats=True)
        else:
            result = await team.run(task=messages)

        # Access the content from the CreateResult object
        return result  # Use the correct attribute to access the generated content
    except Exception as e:
        print(f"Error while refining hypotheses: {e}")
        return TaskResult(
            messages=[
                TextMessage(
                    content="An error occurred while refining hypotheses.",
                    source="system",
                )
            ]
        )
