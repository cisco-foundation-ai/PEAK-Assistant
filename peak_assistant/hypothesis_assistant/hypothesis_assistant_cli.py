#!/usr/bin/env python3
# Copyright (c) 2025 Cisco Systems, Inc. and its affiliates
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.
#
# SPDX-License-Identifier: MIT


import os
import argparse
from dotenv import load_dotenv
import asyncio

from autogen_core.models import UserMessage, SystemMessage

from ..utils import find_dotenv_file
from ..utils.llm_factory import get_model_client


async def hypothesizer(
    user_input: str, research_document: str, local_context: str
) -> str:
    """
    Hypothesizer agent that combines user input, a markdown document, and its own prompt
    to generate output using the configured LLM provider.

    Args:
        user_input (str): A string provided by the user.
        research_document (str): A longer string containing a complete markdown document.
        local_context (str): Additional context that may be relevant to the hypothesis generation.

    Returns:
        str: The output generated by the LLM.
    """
    # Define the system prompt for the hypothesizer
    system_prompt = """
        You are an expert in cybersecurity threat hunting. As the hypothesis agent,
        your task is to generate hypotheses based on the user's input
        and the provided threat hunting behavior or technique research document.
        Use the following guidelines:
        1. Carefully analyze the user's input to understand the context, intent, 
           guidelines, or restrictions they may have.
        2. Extract relevant information from the research document.
        3. Combine both to generate clear, actionable, testable hypotheses.

        Respond with a list of potential hunting hypotheses.

        Your output should be a list of hypotheses, each on a line by itself. There
        should be no additional text, explanations, or formatting. Include at least
        three hypotheses, but feel free to generate more if you can.
        
        If you cannot generate a hypothesis, please say "No hypothesis could be generated."
    """

    messages = [
        SystemMessage(content=system_prompt),
        UserMessage(
            content=f"Here is the research document:\n{research_document}\n",
            source="user",
        ),
        UserMessage(content=f"Here is the user's input: {user_input}\n", source="user"),
        UserMessage(
            content=f"Additional local context: {local_context}\n", source="user"
        ),
    ]

    chat_model_client = await get_model_client("chat")

    # Call the LLM using the configured provider client
    try:
        result = await chat_model_client.create(messages)  # Await the async method
        # Access the content from the CreateResult object
        return str(
            result.content
        )  # Use the correct attribute to access the generated content
    except Exception as e:
        print(f"Error while generating hypotheses: {e}")
        return "An error occurred while generating hypotheses."


def main():
    # Set up argument parser
    parser = argparse.ArgumentParser(
        description="Given a threat hunting technique dossier, generate potential hypotheses for the hunter."
    )
    parser.add_argument("-e", "--environment", help="Path to specific .env file to use")
    parser.add_argument(
        "-r",
        "--research",
        help="Path to the research document (markdown file)",
        required=True,
    )
    parser.add_argument(
        "-u",
        "--user_input",
        help="User input for hypothesis generation",
        required=False,
        default="",
    )
    parser.add_argument(
        "-c",
        "--local_context",
        help="Additional local context to consider",
        required=False,
        default=None,
    )

    args = parser.parse_args()

    # Load environment variables
    if args.environment:
        # Use the specified .env file
        dotenv_path = args.environment
        if not os.path.exists(dotenv_path):
            print(f"Error: Specified environment file '{dotenv_path}' not found")
            exit(1)
        load_dotenv(dotenv_path)
    else:
        # Search for .env file
        dotenv_path = find_dotenv_file()
        if dotenv_path:
            load_dotenv(dotenv_path)
        else:
            print("Warning: No .env file found in current or parent directories")

    # Read the contents of the research document
    try:
        with open(args.research, "r", encoding="utf-8") as file:
            research_data = file.read()
    except FileNotFoundError:
        print(f"Error: Research document '{args.research}' not found")
        exit(1)
    except Exception as e:
        print(f"Error reading research document: {e}")
        exit(1)

    # Read the contents of the local context if provided
    local_context = None
    if args.local_context:
        try:
            with open(args.local_context, "r", encoding="utf-8") as file:
                local_context = file.read()
        except FileNotFoundError:
            print(f"Error: Local context file '{args.local_context}' not found")
            exit(1)
        except Exception as e:
            print(f"Error reading local context: {e}")
            exit(1)

    # Run the hypothesizer asynchronously
    hypotheses = asyncio.run(
        hypothesizer(
            user_input=args.user_input,
            research_document=research_data,
            local_context=local_context,
        )
    )
    print(hypotheses)


if __name__ == "__main__":
    main()
