#!/usr/bin/env python3
# Copyright (c) 2025 Cisco Systems, Inc. and its affiliates
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.
#
# SPDX-License-Identifier: MIT


import os
import sys
import argparse
from typing import List
from dotenv import load_dotenv
import asyncio

from autogen_agentchat.messages import TextMessage
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import TextMentionTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.ui import Console
from autogen_agentchat.base import TaskResult

from ..utils import find_dotenv_file
from ..utils.llm_factory import get_model_client
from ..utils.agent_callbacks import (
    preprocess_messages_logging,
    postprocess_messages_logging,
)
from ..utils.result_extractors import extract_refined_hypothesis


async def refiner(
    hypothesis: str,
    local_context: str,
    research_document: str,
    local_data_document: str,
    verbose: bool = False,
    previous_run: list = list(),
    msg_preprocess_callback=None,
    msg_preprocess_kwargs=None,
    msg_postprocess_callback=None,
    msg_postprocess_kwargs=None,
) -> TaskResult:
    """
    Threat hunting hypothesis refiner agent that combines user input, a markdown document, and its own prompt
    to generate output using the configured LLM provider.

    Args:
        hypothesis (str): A string provided by the user containing a threat hunting hypothesis.
        user_input (str): A string provided by the user containing additional context or information.
        research_document (str): A longer string containing a complete markdown document.

    Returns:
        str: The output generated by the LLM.
    """
    refiner_system_prompt = """
You are a threat hunting hypothesis advisor. Your job is to help the user improve
their existing threat hunting hypothesis into its best form. Based on the provided 
hypothesis, research report about a threat actor or attack technique, any 
available local context, and feedback about how to improve the hypothesis, generate an 
improved version of the hypothesis.

## Requirements:
The improved hypothesis must be:
- Specific: Clearly define the threat actor behavior, technique, or vulnerability
- Measurable: Can be proven or disproven through data analysis
- Achievable: Could realistically be investigated with common security tools and logs
- Relevant: Based on the techniques and behaviors described in the research report

## Critical Constraints:
- Do NOT include time windows or time boundaries
- Do NOT specify data sources, log types, or analysis methods. You may include general 
  references (e.g., "network traffic", "system logs", "authentication logs", "EDR logs") but not specific
  data sources (e.g., "Sysmon event code 1", "Zeek HTTP logs")
- Do NOT number or label the hypotheses
- Focus on WHAT might be happening, not HOW to detect it
- Use generic system descriptions (e.g., "mail servers") unless the threat is specific to 
  particular software (e.g., "Exchange 2019")

## Output Format:
- Return ONLY the hypothesis
- No introductory text, explanations, or conclusions (e.g., do not begin with text like "Based on the 
  provided report and local context, here are ten hypotheses:", "Here are some hypotheses:", etc.)

## Examples of Good Hypotheses:
Threat actors may be using PowerShell Empire to establish persistence on Windows endpoints through scheduled tasks
Attackers may be exploiting unpatched Log4j vulnerabilities in internet-facing applications for initial access
Adversaries may be performing reconnaissance through abnormal LDAP queries against domain controllers
Threat actors may be exfiltrating data through DNS tunneling from database servers

Threat actors may be active in the last 30 days using PowerShell [includes time window]
Check Cisco ASA firewall logs for suspicious traffic to known C2 servers [specifies data source]
Adversaries might be present somewhere in the network [too vague]
Use Splunk to search for base64 encoded commands [specifies tool/method]
Based on the provided report and local context, here are ten hypotheses: [this is explanatory or introductory text not a hypothesis]
4. An unusual spike in failed login attempts from unknown IP addresses might indicate a DDoS attack. [hypothesis is numbered]
Hunt for signs of a data exfiltration attempt using PowerShell Empire. [specifies a task not a hypothesis]
Refined hypothesis: LSASS process memory on Windows systems is being accessed without proper authorization, indicating potential theft of authentication credentials, leading to unauthorized access and data exfiltration. [Begins with a "Refined hypothesis:" label, no details on how LSASS might be being accessed that can be tested]
    """

    critic_system_prompt = """
You are an expert threat hunting hypothesis critic. Evaluate this hypothesis against 8 quality criteria and provide specific, actionable feedback for improvement. Do NOT rewrite the hypothesis - only provide guidance on how to improve it.

---

Evaluate against these 8 criteria:

## 1. ASSERTION QUALITY (0 or 100)
**Question**: Is this stated as a clear assertion about what adversaries ARE DOING (not what detection might show)?

**Check for problems:**
- Uses detection language: "could indicate," "might suggest," "may reveal," "evidence shows"
- Describes investigation steps: "cross-referencing," "systematic review," "hunting for," "investigation into"
- Phrased as a question
- Describes detection outcomes rather than adversary behavior

**Score:**
- 100 if clearly states what adversaries are doing
- 0 if any of the above problems exist

---

## 2. SPECIFICITY (0-100)
**Question**: How many specific qualifiers narrow the scope?

**Count only (maximum 5):**
- Specific technique names (e.g., "Pass-the-Hash", "credential dumping")
- Specific tool names (e.g., "mimikatz.exe", "procdump.exe")
- Specific protocols/mechanisms (e.g., "via SMB", "using WMI")
- Specific system types (e.g., "domain controllers", "endpoints")
- Specific file patterns/indicators (e.g., "lsass.dmp", "0x1410")

**Do NOT count:**
- Generic terms like "custom tools", "suspicious", "various"
- The word "adversaries/attackers/threat actors"
- Tool categories unless naming specific ones
- Vague qualifiers like "unusual", "predictable"

**Score:** Count (0-5) multiplied by 20 = 0, 20, 40, 60, 80, or 100

---

## 3. SCOPE APPROPRIATENESS (0, 50, or 100)
**Question**: Is the scope neither too broad nor too narrow?

**Score 100 if:**
- Focuses on 1-2 specific related behaviors or techniques
- Bounded enough to be actionable
- Broad enough to be meaningful (not just a single IOC)

**Score 50 if:**
- Slightly too broad (covers multiple unrelated techniques)
- Slightly too narrow (very specific but still huntable)

**Score 0 if:**
- Too broad: uses "all", "any", "various types of", "general"
- Too narrow: single IP, single hash, single event
- Unbounded: no clear scope or focus

---

## 4. TECHNICAL PRECISION (0, 50, or 100)
**Question**: Does it use specific technical terms rather than vague language?

**Score 100 if:**
- Uses specific technical terms throughout
- Avoids vague security buzzwords
- No ambiguous language

**Score 50 if:**
- Mix of specific and vague language

**Score 0 if contains vague terms like:**
- "suspicious activity", "anomalous behavior", "unusual patterns"
- "various methods", "different ways", "somehow"
- Generic security terms without specifics

---

## 5. OBSERVABLE FOCUS (0, 50, or 100)
**Question**: Does it describe activities that leave observable evidence?

**Score 100 if describes:**
- Activities that leave evidence (logs, files, network traffic, process artifacts)
- Can be directly observed in security data
- Adversary actions, not detection/hunting methodology
- Technical behaviors (execution, access, creation, transfer)

**Score 50 if:**
- Partially observable activity
- Mixes observable behaviors with investigation methodology

**Score 0 if:**
- Describes investigation processes ("cross-referencing", "systematic review")
- Focuses on analytic techniques rather than adversary behavior
- Describes detection tool operations
- Abstract states with no observable evidence

---

## 6. DETECTION INDEPENDENCE (0 or 100)
**Question**: Is it free from mentions of specific detection products/platforms?

**Score 100 if:**
- No specific detection products/platforms mentioned
- No SIEM, EDR, NDR product names
- Describes behavior independent of detection tooling
- Portable across different environments

**Score 0 if:**
- Mentions specific tools (Splunk, Zeek, QRadar, CrowdStrike, etc.)
- References product-specific features
- Uses phrases like "in Splunk", "using Zeek"

---

## 7. GRAMMATICAL CLARITY (0, 50, or 100)
**Question**: Is the sentence structure clear and concise?

**Score 100 if:**
- Clear, concise sentence structure
- No run-on sentences (under 35 words)
- Straightforward construction
- Minimal nested clauses
- Easy to read on first pass

**Score 50 if:**
- Somewhat complex but readable
- Minor structural issues

**Score 0 if:**
- Run-on sentences (40+ words)
- Multiple nested clauses
- Convoluted structure requiring multiple reads
- Unclear antecedents

---

## 8. LOGICAL COHERENCE (0, 50, or 100)
**Question**: Do all components fit together logically?

**Score 100 if:**
- All components technically compatible
- Technique matches mechanism
- Target systems make sense
- No contradictions

**Score 50 if:**
- Minor inconsistencies but generally coherent
- Slightly unusual but plausible combinations

**Score 0 if:**
- Technical impossibilities
- Mechanism doesn't match technique
- Incompatible components
- Clear contradictions

---

## OUTPUT FORMAT

After evaluating all 8 criteria:

1. Count how many criteria scored below 100
2. Apply decision logic:

**If 0 or 1 criteria scored below 100:**
Output exactly this text and nothing else:

YYY-HYPOTHESIS-ACCEPTED-YYY

**If 2 or more criteria scored below 100:**
Provide feedback organized by criterion name. Only include criteria that scored below 100:

**[CRITERION NAME]:**
- [Specific actionable feedback point 1]
- [Specific actionable feedback point 2]
- [Specific actionable feedback point 3 if needed]

---

## IMPORTANT RULES:
- Count how many criteria are below 100, do NOT try to calculate averages
- If count is 0 or 1: Output "YYY-HYPOTHESIS-ACCEPTED-YYY" only
- If count is 2+: Output feedback bullets for each criterion below 100
- Be specific and actionable in feedback
- Don't rewrite the hypothesis
- Keep feedback concise (1-3 bullets per criterion)
    """

    # Get client for refiner agent (used by both refiner and critic)
    hypothesis_refiner_client = await get_model_client(agent_name="hypothesis-refiner")
    hypothesis_refiner_critic_client = await get_model_client(agent_name="hypothesis-refiner-critic")

    # Create the primary agent.
    refiner_agent = AssistantAgent(
        "refiner", model_client=hypothesis_refiner_client, system_message=refiner_system_prompt
    )

    # Create the critic agent.
    critic_agent = AssistantAgent(
        "critic", model_client=hypothesis_refiner_critic_client, system_message=critic_system_prompt
    )

    # Define a termination condition that stops the task if the critic approves.
    text_termination = TextMentionTermination("YYY-HYPOTHESIS-ACCEPTED-YYY")

    # Create a team with the primary and critic agents.
    team = RoundRobinGroupChat(
        [critic_agent, refiner_agent], termination_condition=text_termination
    )

    # Always add these, no matter if it's the first run or a subsequent one
    messages = [
        TextMessage(
            content=f"Here is the user's hypothesis: {hypothesis}\n", source="user"
        ),
        TextMessage(
            content=f"Here is the research document:\n{research_document}\n",
            source="user",
        ),
        TextMessage(
            content=f"Here is the local data document:\n{local_data_document}\n",
            source="user",
        ),
        TextMessage(
            content=f"Additional local context: {local_context}\n", source="user"
        ),
    ]

    # If we have messages from a previous run, add them so we can continue the research
    if previous_run:
        messages = messages + previous_run

    # Preprocess the messages
    if msg_preprocess_callback:
        messages = msg_preprocess_callback(
            msgs=messages, **(msg_preprocess_kwargs or {})
        )

    try:
        # Run the team asynchronously
        if verbose:
            result = await Console(team.run_stream(task=messages), output_stats=True)
        else:
            result = await team.run(task=messages)

        # Postprocess the result
        if msg_postprocess_callback:
            result = msg_postprocess_callback(
                result=result, **(msg_postprocess_kwargs or {})
            )

        # Access the content from the CreateResult object
        return result  # Use the correct attribute to access the generated content
    except Exception as e:
        print(f"Error while refining hypotheses: {e}")
        raise Exception("An error occurred while refining the hypothesis.") from e


def main() -> None:
    # Set up argument parser
    parser = argparse.ArgumentParser(
        description="Given a threat hunting technique dossier, generate potential hypotheses for the hunter."
    )
    parser.add_argument("-e", "--environment", help="Path to specific .env file to use")
    parser.add_argument(
        "-y", "--hypothesis", help="The hypothesis to be refined", required=True
    )
    parser.add_argument(
        "-r",
        "--research",
        help="Path to the research document (markdown file)",
        required=True,
    )
    parser.add_argument(
        "-l",
        "--local-data",
        help="Path to the local data document (markdown file)",
        required=False,
        default=None,
    )
    parser.add_argument(
        "-c",
        "--local_context",
        help="Additional local context to consider",
        required=False,
        default=None,
    )
    parser.add_argument(
        "-v",
        "--verbose",
        action="store_true",
        help="Enable verbose output",
        default=False,
    )
    parser.add_argument(
        "--no-feedback",
        action="store_true",
        help="Skip user feedback and automatically accept the refined hypothesis",
        default=False,
    )

    # Parse the arguments
    args = parser.parse_args()

    # Enforce verbose behavior based on the no-feedback flag
    if not args.no_feedback:
        # Force verbose to True if not in no-feedback mode
        args.verbose = True

    # Load environment variables
    if args.environment:
        # Use the specified .env file
        dotenv_path = args.environment
        if not os.path.exists(dotenv_path):
            print(f"Error: Specified environment file '{dotenv_path}' not found")
            exit(1)
        load_dotenv(dotenv_path)
    else:
        # Search for .env file
        dotenv_path = find_dotenv_file()
        if dotenv_path:
            load_dotenv(dotenv_path)
        else:
            print("Warning: No .env file found in current or parent directories")

    # Read the contents of the research document
    try:
        with open(args.research, "r", encoding="utf-8") as file:
            research_data = file.read()
    except FileNotFoundError:
        print(f"Error: Research document '{args.research}' not found")
        exit(1)
    except Exception as e:
        print(f"Error reading research document: {e}")
        exit(1)

    # Read the contents of the local data document if provided
    local_data = None
    if args.local_data:
        try:
            with open(args.local_data, "r", encoding="utf-8") as file:
                local_data = file.read()
        except FileNotFoundError:
            print(f"Error: Local data document '{args.local_data}' not found")
            exit(1)
        except Exception as e:
            print(f"Error reading local data document: {e}")
            exit(1)

    # Read the contents of the local context if provided
    local_context = None
    if args.local_context:
        try:
            with open(args.local_context, "r", encoding="utf-8") as file:
                local_context = file.read()
        except FileNotFoundError:
            print(f"Error: Local context file '{args.local_context}' not found")
            exit(1)
        except Exception as e:
            print(f"Error reading local context: {e}")
            exit(1)

    messages: List[TextMessage] = list()
    current_hypothesis = args.hypothesis
    while True:
        # Run the hypothesizer asynchronously
        response = asyncio.run(
            refiner(
                hypothesis=current_hypothesis,
                local_context=local_context or "",
                research_document=research_data,
                local_data_document=local_data or "",
                verbose=args.verbose,
                previous_run=messages,
                msg_preprocess_callback=preprocess_messages_logging,
                msg_preprocess_kwargs={"agent_id": "hypothesis-refiner"},
                msg_postprocess_callback=postprocess_messages_logging,
                msg_postprocess_kwargs={"agent_id": "hypothesis-refiner"},
            )
        )

        # Extract the refined hypothesis using the centralized extractor
        current_hypothesis = extract_refined_hypothesis(response)

        # Print the refined hypothesis and ask for user feedback
        print(f"Hypothesis:\n\n{current_hypothesis}")

        if args.no_feedback:
            # In no-feedback mode, accept the first refinement and exit
            print(
                "No-feedback mode: Hypothesis refinement completed.", file=sys.stderr
            )
            break
        
        feedback = input(
            "Please provide your feedback on the refined hypothesis (or press Enter to approve it): "
        )

        if feedback.strip():
            # If feedback is provided, add it to the messages and loop back to the refiner
            messages.append(
                TextMessage(content=f"User feedback: {feedback}\n", source="user")
            )
        else:
            break


if __name__ == "__main__":
    main()
