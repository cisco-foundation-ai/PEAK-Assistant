from autogen_agentchat.base import TaskResult
from autogen_agentchat.messages import TextMessage
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.ui import Console
from autogen_agentchat.conditions import TextMentionTermination

from utils.assistant_auth import PEAKAssistantAuthManager
from utils.azure_client import PEAKAssistantAzureOpenAIClient
from utils.mcp_config import get_client_manager, setup_mcp_servers


async def identify_data_sources(
    hypothesis: str,
    research_document: str,
    able_info: str,
    local_context: str,
    verbose: bool = False,
    previous_run: list = list(),
    mcp_server_group: str = "data_discovery",
    msg_preprocess_callback = None,
    msg_preprocess_kwargs = None,
    msg_postprocess_callback = None,
    msg_postprocess_kwargs = None    
) -> TaskResult:
    """
    Data agent that consumes a hunting research report and a hypothesis, then
    queries the Splunk server to try to identify data sources that could be used
    to test the hypothesis.

    Args:
        hypothesis (str): A string containing the hypothesis to be tested.
        research_document (str): A longer string containing a complete markdown document.

    Returns:
        str: The output generated by the LLM.
    """
    # Define the system prompt for the hypothesizer
    data_discovery_prompt = """
        You are an expert in cybersecurity threat hunting. As the data discovery agent,
        your task is examine the provided hypothesis and the threat hunting behavior or 
        technique research document, then identify potential Splunk indices or data sources 
        that could be used to test the hypothesis from my actual Splunk server.

        Keep in mind the following guidelines:
        - Carefully analyze the hypothesis to understand its context and requirements.
        - Extract relevant information from the research document.
        - Combine both to identify specific Splunk indices or data sources that would be useful
          for testing the hypothesis. 
        - You must actually inspect the events in every index and their fields in Splunk to ensure they
          contain relevant data.
        - The actual index or field names in Splunk may not match the names in the research document. 
          You may need to inspect data in Splunk to find the correct names.
        - If the exact type of data you expect is not availalble, it is possible that there is a
          similar type of data that could be used instead.
        - Do not report indices, sourcetypes or fields that are not relevant. For example,
          it is not necessary to report indices that contain data that is not relevant or
          that contain no data at all. Only report data sources that might be helpful 
          to the hunter.

        Respond with a list of potential Splunk indices or data sources, a brief description of the type
        of data they contain (high level, such as "Windows Event Logs" or "Linux authentication events" is OK)
        as well as a description of the key fields, or the the fields most likely to contain the 
        data needed to test the hypothesis. All of this should be in markdown table format, 
        though you may include brief notes in markdown format if necessary.

        Only present your conclusions in the response, do not include any other text. Do not offer
        to provide more information or suggest additional tasks.
        
        When receiving feedback from a verifier agent, use your tools to iteratively refine
        your understanding of the data sources and fields and their relevance to the 
        hypothesis. 

        If you cannot identify an appropriate data source, please respond with "No suitable data sources found."
        followed by a brief explanation of why no data sources were identified and the closest data source(s) 
        you could find.
    """

    discovery_critic_prompt = """
        You are an expert in cybersecurity threat hunting. As the data discovery critic agent,
        your task is to review the output of the data discovery agent and provide feedback on the
        identified Splunk indices or data sources. You must ensure that the identified data
        sources are relevant to the hypothesis and the research document. Don't hesitate to
        ask the data discovery agent to refine its understanding of the data sources
        and fields and their relevance to the hypothesis if necessary.

        Remember to be on the lookout for the following:
        - Ensure that the identified data sources are relevant to the hypothesis and the research document.
        - If the data discovery agent has not identified all the relevant data sources and fields,
          provide constructive feedback to help it refine its understanding.
        - If the data discovery agent has identified some relevant data sources but not all,
          ask it to iterate and explore further.
        - Ensure that the data discovery agent is not making assumptions about the data 
          source contents based on the research document alone. It must actually inspect 
          the data in Splunk. Random samples are usually good enough to determine
          whether the data source is relevant or not.

        If you believe the data discovery agent has identified all the relevant data sources and fields,
        respond with "YYY-TERMINATE-YYY" to indicate that the data discovery agent can stop iterating.
    """

    messages = [
        TextMessage(
            content=f"Here is the research document:\n{research_document}\n",
            source="user",
        ),
        TextMessage(content=f"Here is the hypothesis: {hypothesis}\n", source="user"),
        TextMessage(
            content=f"The Actor, Behavior, Location and Evidence (ABLE) information is as follows: {able_info}",
            source="user",
        ),
        TextMessage(
            content=f"Additional local context: {local_context}\n", source="user"
        ),
    ]

    # If we have messages from a previous run, add them so we can continue the research
    if previous_run:
        messages = messages + previous_run

    # Initialize the model client
    auth_mgr = PEAKAssistantAuthManager()
    az_model_client = await PEAKAssistantAzureOpenAIClient().get_client(
        auth_mgr=auth_mgr
    )
    az_model_reasoning_client = await PEAKAssistantAzureOpenAIClient().get_client(
        auth_mgr=auth_mgr, model_type="reasoning"
    )

    # Set up MCP servers for data discovery
    mcp_client_manager = get_client_manager()
    connected_servers = await setup_mcp_servers(mcp_server_group)

    if not connected_servers:
        error_msg = f"No MCP servers could be connected from group '{mcp_server_group}'. Check your MCP configuration."
        if verbose:
            print(error_msg)
        raise RuntimeError(error_msg)

    if verbose:
        print(
            f"Connected to {len(connected_servers)} MCP servers for data discovery: {', '.join(connected_servers)}"
        )

    # Get workbenches only from the data discovery server group
    group_workbenches = []
    for server_name in connected_servers:
        workbench = mcp_client_manager.get_workbench(server_name)
        if workbench:
            group_workbenches.append(workbench)

    if not group_workbenches:
        error_msg = f"No MCP workbenches available for data discovery group '{mcp_server_group}'. Check your MCP configuration."
        if verbose:
            print(error_msg)
        raise RuntimeError(error_msg)

    # Use the first workbench from the data discovery group
    mcp_workbench = group_workbenches[0]
    return await _run_data_discovery_with_workbench(
        mcp_workbench,
        messages,
        data_discovery_prompt,
        discovery_critic_prompt,
        az_model_client,
        az_model_reasoning_client,
        verbose,
        previous_run,
        msg_preprocess_callback,
        msg_preprocess_kwargs,
        msg_postprocess_callback,
        msg_postprocess_kwargs
    )


async def _run_data_discovery_with_workbench(
    mcp_workbench,
    messages,
    data_discovery_prompt,
    discovery_critic_prompt,
    az_model_client,
    az_model_reasoning_client,
    verbose,
    previous_run,
    msg_preprocess_callback,
    msg_preprocess_kwargs,
    msg_postprocess_callback,
    msg_postprocess_kwargs
) -> TaskResult:
    """Helper function to run data discovery with a given MCP workbench"""

    data_discovery_agent = AssistantAgent(
        "Data_Discovery_Agent",
        model_client=az_model_client,
        workbench=mcp_workbench,
        reflect_on_tool_use=True,
        model_client_stream=True,
        system_message=data_discovery_prompt,
    )

    discovery_critic_agent = AssistantAgent(
        "Discovery_Critic_Agent",
        model_client=az_model_reasoning_client,
        system_message=discovery_critic_prompt,
    )

    # Define a termination condition that stops the task once the summarizer
    # agent has completed its task
    text_termination = TextMentionTermination("YYY-TERMINATE-YYY")

    team = RoundRobinGroupChat(
        participants=[data_discovery_agent, discovery_critic_agent],
        termination_condition=text_termination,
    )

    # Preprocess the messages
    if msg_preprocess_callback:
        messages = msg_preprocess_callback(msgs=messages, **(msg_preprocess_kwargs or {}))

    try:
        if verbose:
            result = await Console(team.run_stream(task=messages))
        else:
            result = await team.run(task=messages)

        # Postprocess the result
        if msg_postprocess_callback: 
            result = msg_postprocess_callback(result=result, **(msg_postprocess_kwargs or {}))  

        return result
    except Exception as e:
        print(f"Error during data source identification: {e}")
        return TaskResult(
            messages=[
                TextMessage(
                    content=f"An error occurred during data source identification: {e}",
                    source="system",
                )
            ]
        )
