# Research Agent Team Evaluator

Comprehensive evaluation system for threat hunting research reports generated by PEAK Assistant's research agent team.

## Overview

The Research Agent Team Evaluator assesses the quality of threat hunting research reports across 13 detailed criteria, providing statistical analysis and multi-backend comparison capabilities. It includes both report generation and evaluation components.

### Evaluation Criteria (Brief)

1. **Structure Compliance** (1.5x) - Required sections present and properly formatted
2. **Technical Depth** (2.0x) - Level of technical detail and code examples
3. **Technical Accuracy** (2.0x) - Correctness of commands, paths, and technical claims
4. **MITRE Coverage** (1.5x) - Quality of MITRE ATT&CK references
5. **Detection Quality** (2.0x) - Quality and actionability of detection methods
6. **Dataset Documentation** (1.8x) - Quality of log source and field documentation
7. **Threat Actor Specificity** (1.0x) - Specificity of threat actor information
8. **Reference Quality** (1.3x) - Quality and diversity of references
9. **URL Validity** (1.5x) - Validity and relevance of URLs
10. **Log Example Quality** (1.7x) - Quality and usefulness of log examples
11. **Instruction Clarity** (1.8x) - Clarity and replicability of instructions
12. **Cross-Section Consistency** (1.2x) - Internal consistency across sections
13. **Tool Documentation** (1.0x) - Quality of tool descriptions

## Workflow

### Complete Evaluation Workflow

The research evaluator follows a two-phase workflow:

#### Phase 1: Generate Reports

Use `generate-reports.py` to create research reports for multiple topics:

```bash
# 1. Create a topics file
cat > topics.txt << EOF
dll side-loading via trusted executables
credential dumping from LSASS
lateral movement via WMI
EOF

# 2. Generate reports with configuration A
python generate-reports.py -i topics.txt -o output-config-a

# 3. (Optional) Generate reports with configuration B for comparison
python generate-reports.py -i topics.txt -o output-config-b
```

**Output structure:**
```
output-config-a/
â”œâ”€â”€ dll side-loading via trusted executables.md
â”œâ”€â”€ credential dumping from LSASS.md
â”œâ”€â”€ lateral movement via WMI.md
â”œâ”€â”€ config-a.json          # JSONL mapping file
â””â”€â”€ logs/
    â”œâ”€â”€ 0001_dll-side-loading.log
    â”œâ”€â”€ 0002_credential-dumping.log
    â””â”€â”€ 0003_lateral-movement.log
```

#### Phase 2: Evaluate Reports

Use `evaluator.py` to assess report quality:

```bash
# Single configuration evaluation
python evaluator.py -i output-config-a/config-a.json

# Multi-configuration comparison
cat output-config-a/config-a.json output-config-b/config-b.json > combined.json
python evaluator.py -i combined.json -v
```

### Input Formats

#### Topics File (for generate-reports.py)

Plain text file with one topic per line:

```text
# Comments start with #
dll side-loading via trusted executables
credential dumping from LSASS

# Blank lines are ignored
lateral movement via WMI
```

#### JSONL File (for evaluator.py)

JSON Lines format with base64-encoded reports:

```json
{"topic": "dll side-loading", "backend": "config-a", "report": "IyBUaHJlYXQgSHVudGluZy..."}
{"topic": "credential dumping", "backend": "config-a", "report": "IyBUaHJlYXQgSHVudGluZy..."}
```

**Fields:**
- `topic` (string) - The hunt topic
- `backend` (string) - Configuration/backend identifier (derived from output directory name)
- `report` (string) - Base64-encoded markdown report content

### Output Formats

#### Console Output

**Single Mode:**
```
Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:45<00:00, 3.5s/metric]

# Evaluation Complete

## Evaluation Summary

**Total reports evaluated:** 3

### Backend: config-a
- Reports: 3
- Average score: 84.5
- Min score: 78.2
- Max score: 89.1
```

**Comparison Mode:**
```
Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [02:15<00:00, 3.5s/metric]

============================================================
OVERALL RANKINGS
============================================================
Total topics evaluated: 3

Final Standings:
  1. config-a: 2 wins, 1 second place (avg: 85.3) ðŸ†
  2. config-b: 1 wins, 2 second place (avg: 82.1)

Metric Champions Across All Topics:
  â­ technical_accuracy: config-a (avg 95.0)
  â­ detection_quality: config-b (avg 88.5)
  ...
```

#### JSONL Output (output_results.jsonl)

One JSON object per topic:

```json
{
  "topic": "dll side-loading",
  "rankings": [
    ["config-a", 85.3],
    ["config-b", 82.1]
  ],
  "winner": "config-a",
  "backends": {
    "config-a": {
      "total_score": 85.3,
      "metrics": {
        "technical_accuracy": {"score": 95.0, "feedback": "..."},
        ...
      }
    }
  }
}
```

#### Full JSON Output (evaluation_full.json)

Complete evaluation data including:
- Metadata (date, models used, configuration)
- All individual evaluations with detailed scores
- Statistical summaries
- Comparison results

## Usage Examples

### Basic Usage

Evaluate a single configuration:

```bash
python evaluator.py -i reports/my-config.json
```

### Compare Multiple Configurations

```bash
# Generate reports with different configs
python generate-reports.py -i topics.txt -o reports/gpt4
python generate-reports.py -i topics.txt -o reports/claude-opus

# Combine and evaluate
cat reports/gpt4/gpt4.json reports/claude-opus/claude-opus.json > combined.json
python evaluator.py -i combined.json -v
```

### Verbose Mode

Show detailed per-metric scores for each report:

```bash
python evaluator.py -i combined.json -v
```

Output includes:
```
## Evaluating config-a report for 'dll side-loading':
Metric                         Score   Weight Feedback
------------------------------ -------- -------- ----------------------------------------
structure_compliance             100.0    x1.5 All required sections present...
technical_depth                   92.0    x2.0 Good technical detail with code...
technical_accuracy                95.0    x2.0 Generally accurate with minor...
...
```

### Custom Output Files

```bash
python evaluator.py \
  -i input.json \
  -o my-results.jsonl \
  --log my-eval.log \
  -j my-full-results.json
```

### Quiet Mode

Suppress console output (only write to files):

```bash
python evaluator.py -i input.json -q
```

## Command-Line Options

### generate-reports.py

| Option | Description | Default |
|--------|-------------|---------|
| `-i, --input FILE` | Topics file (one per line) | **Required** |
| `-o, --output DIR` | Output directory for reports and JSONL | **Required** |
| `--assistant-cmd CMD` | Command to invoke research assistant | `research-assistant` |
| `--max-retries N` | Max retries on failure | `5` |
| `--retry-wait SEC` | Base wait seconds before retry | `60` |
| `--backoff-factor N` | Backoff multiplier between retries | `2.0` |
| `--timeout-seconds SEC` | Per-attempt timeout | `900` (15 min) |
| `--force` | Overwrite existing report files | `False` |

### evaluator.py

| Option | Description | Default |
|--------|-------------|---------|
| `-i, --input FILE` | JSONL file with reports | `input_reports.jsonl` |
| `-o, --output FILE` | Output JSONL file | `output_results.jsonl` |
| `-c, --model-config FILE` | Path to model_config.json | `model_config.json` |
| `-l, --log FILE` | Log file for console output | `evaluation_log.txt` |
| `-j, --json-output FILE` | Full JSON output file | `evaluation_full.json` |
| `--no-json` | Disable full JSON output | `False` |
| `-v, --verbose` | Verbose output with per-metric details | `False` |
| `-q, --quiet` | Quiet mode (no console output) | `False` |
| `--raw` | Print raw Markdown (disable rich rendering) | `False` |

## Model Configuration

### Judge Roles and Weights

The research evaluator uses 13 judge roles plus 1 utility role:

| Judge Role | Weight | Category | Recommended Model |
|------------|--------|----------|-------------------|
| `technical_depth` | 2.0 | High | Critical/Quality |
| `technical_accuracy` | 2.0 | High | Critical/Quality |
| `detection_quality` | 2.0 | High | Critical/Quality |
| `dataset_documentation` | 1.8 | Medium-High | Quality |
| `instruction_clarity` | 1.8 | Medium-High | Quality |
| `log_example_quality` | 1.7 | Medium-High | Quality |
| `structure_compliance` | 1.5 | Medium | Standard |
| `mitre_coverage` | 1.5 | Medium | Standard |
| `url_validity` | 1.5 | Medium | Standard |
| `reference_quality` | 1.3 | Medium | Standard |
| `cross_section_consistency` | 1.2 | Low | Standard |
| `threat_actor_specificity` | 1.0 | Low | Fast |
| `tool_documentation` | 1.0 | Low | Fast |
| `section_extractor` | N/A | Utility | Quality |

### Example Configuration

See `model_config.json.example` for a complete example:

```json
{
  "version": "1",
  "providers": {
    "anthropic-main": {
      "type": "anthropic",
      "config": {
        "api_key": "${ANTHROPIC_API_KEY}"
      }
    }
  },
  "defaults": {
    "provider": "anthropic-main",
    "model": "claude-sonnet-4-20250514"
  },
  "groups": {
    "high-weight-metrics": {
      "match": ["technical_depth", "technical_accuracy", "detection_quality"],
      "model": "claude-opus-4-1-20250805",
      "comment": "Use best model for weight 2.0 metrics"
    },
    "medium-weight-metrics": {
      "match": ["dataset_documentation", "instruction_clarity", "log_example_quality"],
      "model": "claude-sonnet-4-20250514",
      "comment": "Use quality model for weight 1.7-1.8 metrics"
    },
    "standard-metrics": {
      "match": ["structure_compliance", "mitre_coverage", "url_validity", 
                "reference_quality", "cross_section_consistency"],
      "model": "claude-sonnet-4-20250514",
      "comment": "Use quality model for weight 1.2-1.5 metrics"
    },
    "fast-metrics": {
      "match": ["threat_actor_specificity", "tool_documentation"],
      "model": "claude-3-5-haiku-20241022",
      "comment": "Use fast model for weight 1.0 metrics"
    },
    "utility": {
      "match": ["section_extractor"],
      "model": "claude-sonnet-4-20250514",
      "comment": "Section extraction utility"
    }
  }
}
```

## Evaluation Criteria Details

### 1. Structure Compliance (Weight: 1.5)

**What it evaluates:** Whether all required sections are present with proper formatting.

**Required sections:**
- Overview
- Threat Actors
- Technique Details
- Detection
- Typical Datasets
- Published Hunts
- Previous Hunting Information
- Commonly-Used Tools
- References
- Other Information

**Scoring rubric:**
- **90-100**: All required sections present with proper markdown headers
- **70-89**: Most sections present, minor formatting issues
- **50-69**: Several sections missing or poorly formatted
- **0-49**: Many required sections missing

**Example feedback:**
- âœ… "All 10 required sections present with proper formatting"
- âš ï¸ "Missing 'Published Hunts' section, otherwise complete"
- âŒ "Only 6 of 10 required sections found"

### 2. Technical Depth (Weight: 2.0)

**What it evaluates:** Level of technical detail including code examples, commands, and specific artifacts.

**Scoring rubric:**
- **90-100**: Rich technical detail with multiple code blocks, specific commands, and artifact examples
- **70-89**: Good technical content with some code examples and specifics
- **50-69**: Basic technical information, limited examples
- **0-49**: Superficial or generic technical content

**Example feedback:**
- âœ… "Excellent depth with 8 code blocks, 12 specific commands, and detailed artifact analysis"
- âš ï¸ "Good technical content but could use more specific examples"
- âŒ "Lacks technical depth, mostly high-level descriptions"

### 3. Technical Accuracy (Weight: 2.0)

**What it evaluates:** Correctness of commands, file paths, registry keys, and technical claims.

**Scoring rubric:**
- **90-100**: All technical details accurate and correct
- **70-89**: Mostly accurate with minor errors
- **50-69**: Several technical errors or questionable claims
- **0-49**: Significant technical inaccuracies

**Example feedback:**
- âœ… "All commands, paths, and technical details verified as accurate"
- âš ï¸ "Minor PowerShell syntax error in one example, otherwise accurate"
- âŒ "Multiple incorrect registry paths and invalid command syntax"

### 4. MITRE Coverage (Weight: 1.5)

**What it evaluates:** Quality and completeness of MITRE ATT&CK references.

**Scoring rubric:**
- **90-100**: Multiple valid MITRE IDs with URLs, proper context, and sub-techniques
- **70-89**: Valid MITRE IDs with some context
- **50-69**: MITRE IDs present but minimal context or some invalid
- **0-49**: No MITRE coverage or mostly invalid IDs

**Example feedback:**
- âœ… "Excellent MITRE coverage: T1055.001, T1055.002 with URLs and detailed context"
- âš ï¸ "Valid MITRE IDs but missing sub-technique specificity"
- âŒ "No MITRE ATT&CK references found"

### 5. Detection Quality (Weight: 2.0)

**What it evaluates:** Quality, actionability, and completeness of detection methods.

**Scoring rubric:**
- **90-100**: Multiple actionable queries (Splunk/KQL/Sigma/YARA), discusses false positives and detection gaps, provides thresholds
- **70-89**: Actionable queries provided, some discussion of limitations
- **50-69**: Basic detection guidance, queries need refinement
- **0-49**: No actionable detection methods or missing section

**Example feedback:**
- âœ… "Excellent detection section with 3 Splunk queries, Sigma rule, false positive analysis, and detection gap discussion"
- âš ï¸ "Good queries but lacks false positive and detection gap discussion"
- âŒ "Detection section too generic, no actionable queries"

### 6. Dataset Documentation (Weight: 1.8)

**What it evaluates:** Quality of log source and field documentation.

**Scoring rubric:**
- **90-100**: Multiple log sources with field explanations, examples, and documentation links to vendor docs or schemas
- **70-89**: Good log source coverage with field descriptions
- **50-69**: Basic dataset information, limited field details
- **0-49**: Minimal or no dataset documentation

**Example feedback:**
- âœ… "Comprehensive dataset documentation: Sysmon, Windows Security, EDR with field mappings and links to Microsoft documentation"
- âš ï¸ "Good log sources identified but missing field-level details and documentation links"
- âŒ "Dataset section lacks specific log sources and fields"

### 7. Threat Actor Specificity (Weight: 1.0)

**What it evaluates:** Specificity and detail of threat actor information.

**Scoring rubric:**
- **90-100**: Names specific threat groups with usage details and context
- **70-89**: Mentions threat actors with some specificity
- **50-69**: Generic threat actor references
- **0-49**: No threat actor information or only generic mentions

**Example feedback:**
- âœ… "Names APT29, APT28 with specific campaign details and TTPs"
- âš ï¸ "Mentions threat actors but lacks specific campaign details"
- âŒ "Only generic 'advanced threat actors' mentioned"

### 8. Reference Quality (Weight: 1.3)

**What it evaluates:** Quality, diversity, and relevance of references.

**Scoring rubric:**
- **90-100**: Diverse, authoritative references (vendor blogs, research papers, MITRE)
- **70-89**: Good references with some diversity
- **50-69**: Limited references or mostly single-source
- **0-49**: No references or poor quality sources

**Example feedback:**
- âœ… "Excellent references: 5 vendor blogs, 2 research papers, MITRE documentation"
- âš ï¸ "Good references but could use more diversity"
- âŒ "Only 1 reference provided, needs more sources"

### 9. URL Validity (Weight: 1.5)

**What it evaluates:** Validity and accessibility of URLs in the report.

**Scoring rubric:**
- **90-100**: All URLs valid and accessible (HTTP 200)
- **70-89**: Most URLs valid, some minor issues
- **50-69**: Several broken or inaccessible URLs
- **0-49**: Many broken URLs or no URLs provided

**Example feedback:**
- âœ… "All 8 URLs validated and accessible"
- âš ï¸ "7 of 9 URLs valid, 2 returned 404"
- âŒ "5 of 8 URLs broken or inaccessible"

**Note:** This metric performs async HTTP checks and may take longer.

### 10. Log Example Quality (Weight: 1.7)

**What it evaluates:** Quality and usefulness of log examples.

**Scoring rubric:**
- **90-100**: Multiple log examples with field explanations and comparisons (benign vs malicious)
- **70-89**: Good log examples with explanations
- **50-69**: Basic log examples, limited context
- **0-49**: No log examples or poor quality

**Example feedback:**
- âœ… "Excellent log examples with field-by-field analysis and benign/malicious comparison"
- âš ï¸ "Good log examples but missing field explanations"
- âŒ "No log examples provided"

### 11. Instruction Clarity (Weight: 1.8)

**What it evaluates:** Clarity and replicability of hunting instructions.

**Scoring rubric:**
- **90-100**: Clear, step-by-step instructions that are easily replicable
- **70-89**: Good instructions with minor ambiguities
- **50-69**: Instructions present but unclear or incomplete
- **0-49**: No clear instructions or not replicable

**Example feedback:**
- âœ… "Crystal clear step-by-step instructions with rationale for each step"
- âš ï¸ "Good steps but could explain 'why' more clearly"
- âŒ "Instructions too vague to follow"

### 12. Cross-Section Consistency (Weight: 1.2)

**What it evaluates:** Internal consistency across different sections.

**Scoring rubric:**
- **90-100**: All sections consistent with each other
- **70-89**: Mostly consistent, minor discrepancies
- **50-69**: Some contradictions between sections
- **0-49**: Significant inconsistencies

**Example feedback:**
- âœ… "All sections internally consistent"
- âš ï¸ "Minor inconsistency: Detection section mentions T1055.003 but Technique Details doesn't"
- âŒ "Significant contradictions between sections"

### 13. Tool Documentation (Weight: 1.0)

**What it evaluates:** Quality of tool descriptions in the "Commonly-Used Tools" section.

**Scoring rubric:**
- **90-100**: Multiple tools with descriptions and usage context
- **70-89**: Tools listed with basic descriptions
- **50-69**: Tools listed but minimal descriptions
- **0-49**: No tools documented or section missing

**Example feedback:**
- âœ… "10 tools documented with descriptions and usage context"
- âš ï¸ "Tools listed but descriptions could be more detailed"
- âŒ "Tools section empty or missing"

## Troubleshooting

### generate-reports.py Issues

**"research-assistant command not found"**
- Solution: Ensure PEAK Assistant is installed and `research-assistant` is in your PATH
- Or use: `--assistant-cmd /path/to/research-assistant`

**Reports timing out**
- Solution: Increase timeout with `--timeout-seconds 1800` (30 minutes)

**All reports failing**
- Check `output-dir/logs/` for detailed error logs
- Verify your PEAK Assistant configuration is working

### evaluator.py Issues

**"Failed to decode report"**
- Solution: Ensure JSONL file has properly base64-encoded reports
- Check that `generate-reports.py` completed successfully

**"429 Rate Limit" errors**
- Solution: The evaluator will automatically retry with backoff (5s, 10s, 20s)
- If persistent, wait a few minutes or use cheaper models

**Metrics scoring 0**
- Check verbose output (`-v`) to see error details
- Verify your model configuration is correct
- Check log file for API errors

## Advanced Usage

### Custom Backend Names

The backend name is derived from the output directory name:

```bash
# Backend will be "gpt4-turbo"
python generate-reports.py -i topics.txt -o reports/gpt4-turbo

# Backend will be "claude-opus-experiment-1"
python generate-reports.py -i topics.txt -o reports/claude-opus-experiment-1
```

### Combining Multiple Runs

```bash
# Generate reports with 3 different configurations
python generate-reports.py -i topics.txt -o run1-baseline
python generate-reports.py -i topics.txt -o run2-optimized
python generate-reports.py -i topics.txt -o run3-experimental

# Combine all JSONL files
cat run1-baseline/run1-baseline.json \
    run2-optimized/run2-optimized.json \
    run3-experimental/run3-experimental.json > comparison.json

# Evaluate all three
python evaluator.py -i comparison.json -v
```

### Statistical Analysis

The evaluator provides comprehensive statistics in comparison mode:
- Mean, median, trimmed mean scores
- 95% confidence intervals
- Consistency scores
- Outlier detection
- Per-metric champions
- Reliability assessment

Access full statistics in the JSON output file.

## See Also

- [Main Evaluations README](../README.md) - Common configuration and setup
- [Hypothesis Evaluator](../hypothesis-eval/README.md) - Evaluate hypotheses
- [Hunt Plan Evaluator](../hunt-plan-eval/README.md) - Evaluate hunt plans
- `model_config.json.example` - Example configuration file
