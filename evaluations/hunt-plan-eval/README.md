# Hunt Plan Evaluator

Evaluation system for threat hunting plan documents generated by PEAK Assistant's planner agent.

## Overview

The Hunt Plan Evaluator assesses the quality and completeness of hunt plan documents across 10 criteria with tiered weights. It supports both single-plan evaluation and side-by-side comparison of two plans.

### Evaluation Criteria (Brief)

**Tier 1 (Weight: 2.5) - Critical:**
1. **Technical Accuracy** - Correctness of SPL syntax, field names, and index references
2. **Query Efficiency** - Use of tstats, efficient filtering, and proper scoping
3. **Organization & Progression** - Logical flow and step dependencies
4. **Template Conformance** - Required sections and proper formatting

**Tier 2 (Weight: 2.0) - Important:**
5. **Hypothesis Alignment** - Coverage of the original hypothesis
6. **Actionability & Clarity** - Executable queries with clear thresholds
7. **Environmental Integration** - Local context and exclusions

**Tier 3 (Weight: 1.5) - Standard:**
8. **Operational Practicality** - Timeframe fit and data volume considerations

**Tier 4 (Weight: 1.2) - Supporting:**
9. **Comprehensiveness** - ABLE framework utilization and data source coverage
10. **Threat Intel Integration** - Actor TTPs and technique references

## Input Format

### Markdown Hunt Plans

Hunt plans must be in Markdown format with the following required sections:

```markdown
# Hunt Plan: [Topic]

## Hypothesis
[The threat hunting hypothesis]

## Recommended Time Frame
[Time period to search]

## ABLE Table
[Assess, Build, Learn, Evaluate framework]

## Data
[Required data sources and fields]

## Hunt Procedure
[Step-by-step hunting instructions with queries]
```

**Example:**
```markdown
# Hunt Plan: Credential Dumping from LSASS

## Hypothesis
Adversaries are dumping credentials from LSASS memory using tools like Mimikatz.

## Recommended Time Frame
Last 30 days

## ABLE Table
| Phase | Description |
|-------|-------------|
| Assess | Review LSASS access patterns |
| Build | Create detection queries |
| Learn | Analyze results |
| Evaluate | Tune and refine |

## Data
- Sysmon Event ID 10 (Process Access)
- Windows Security Event ID 4688 (Process Creation)

## Hunt Procedure
### Step 1: Identify LSASS Access
```spl
index=windows sourcetype=XmlWinEventLog:Microsoft-Windows-Sysmon/Operational EventCode=10 
TargetImage="*\\lsass.exe"
| stats count by SourceImage, SourceUser
```
```

## Output Formats

### Console Output

**Single Plan:**
```markdown
# Evaluation Complete

## Evaluating: plan1.md

| Metric | Score | Weight | Feedback |
| :-- | --: | --: | :-- |
| technical_accuracy | 95.0 | x2.5 | Excellent SPL syntax... |
| query_efficiency | 88.0 | x2.5 | Good use of tstats... |
...
| TOTAL SCORE | 87.3 | | |

## Summary
**File:** plan1.md
**Total Score:** 87.3/100
**Weighted Score:** 87.3
```

**Comparison Mode:**
```markdown
# Hunt Plan Comparison

## Winner: plan1.md (87.3)

### Rankings
1. plan1.md: 87.3 üèÜ
2. plan2.md: 82.1

### Key Differences
- **technical_accuracy**: plan1.md better by 8.0 points
- **query_efficiency**: plan1.md better by 5.2 points
- **roughly equal**: hypothesis_alignment, actionability_clarity

### Metric Breakdown
| Metric | plan1.md | plan2.md | Diff |
|--------|----------|----------|------|
| technical_accuracy | 95.0 | 87.0 | +8.0 |
| query_efficiency | 88.0 | 82.8 | +5.2 |
...
```

### JSON Output

**Single plan (hunt-plan-compare.json):**
```json
{
  "metadata": {
    "evaluation_date": "2025-01-09 12:00:00",
    "model_config": "model_config.json",
    "models_used": {
      "technical_accuracy": "anthropic:claude-opus-4-1-20250805",
      ...
    },
    "files": ["plan1.md"]
  },
  "evaluations": [
    {
      "filename": "plan1.md",
      "total_score": 87.3,
      "metrics": {
        "technical_accuracy": {
          "score": 95.0,
          "weight": 2.5,
          "feedback": "...",
          "confidence": 0.9
        },
        ...
      }
    }
  ],
  "comparison": null
}
```

**Comparison (hunt-plan-compare.json):**
```json
{
  "metadata": { ... },
  "evaluations": [ ... ],
  "comparison": {
    "winner": "plan1.md",
    "rankings": [
      {"file": "plan1.md", "score": 87.3},
      {"file": "plan2.md", "score": 82.1}
    ],
    "key_differences": [
      "technical_accuracy: plan1.md better by 8.0 points",
      ...
    ]
  }
}
```

### Full JSON Output

Detailed JSON with complete evaluation data (use `-j` flag):
```json
{
  "metadata": { ... },
  "evaluations": [
    {
      "filename": "plan1.md",
      "total_score": 87.3,
      "weighted_score": 87.3,
      "metrics": {
        "technical_accuracy": {
          "score": 95.0,
          "weight": 2.5,
          "feedback": "Excellent SPL syntax with proper field names...",
          "confidence": 0.9,
          "details": {
            "spl_queries_found": 5,
            "syntax_errors": 0,
            ...
          }
        },
        ...
      }
    }
  ],
  "comparison": { ... }
}
```

## Usage Examples

### Basic Usage

Evaluate a single hunt plan:

```bash
python hunt_plan_evaluator.py plan.md
```

### Compare Two Plans

```bash
python hunt_plan_evaluator.py plan1.md plan2.md
```

### Custom Output Files

```bash
python hunt_plan_evaluator.py plan1.md plan2.md \
  --output comparison.json \
  --log evaluation.log \
  -j full-details.json
```

### Specify Model Config

```bash
python hunt_plan_evaluator.py plan.md -c /path/to/model_config.json
```

### Disable Rich Rendering

```bash
python hunt_plan_evaluator.py plan.md --raw
```

### Quiet Mode

```bash
python hunt_plan_evaluator.py plan.md -q
```

## Command-Line Options

| Option | Description | Default |
|--------|-------------|---------|
| `files` | Markdown files to evaluate (1 or 2) | **Required** |
| `-c, --model-config FILE` | Path to model_config.json | `model_config.json` |
| `--output FILE` | Output JSON file | `hunt-plan-compare.json` |
| `--log FILE` | Log file for console output | `hunt-plan-compare.log` |
| `-j, --json-output FILE` | Full JSON with complete details | `hunt-plan-compare.full.json` |
| `--no-json` | Disable full JSON output | `False` |
| `--raw` | Print raw Markdown (disable rich) | `False` |
| `-q, --quiet` | Quiet mode (no console output) | `False` |

## Model Configuration

### Judge Roles and Weights

The hunt plan evaluator uses 10 judge roles plus 1 utility role:

| Judge Role | Weight | Tier | Recommended Model |
|------------|--------|------|-------------------|
| `technical_accuracy` | 2.5 | Tier 1 | Critical |
| `query_efficiency` | 2.5 | Tier 1 | Critical |
| `organization_progression` | 2.5 | Tier 1 | Critical |
| `template_conformance` | 2.5 | Tier 1 | Critical |
| `hypothesis_alignment` | 2.0 | Tier 2 | Quality |
| `actionability_clarity` | 2.0 | Tier 2 | Quality |
| `environmental_integration` | 2.0 | Tier 2 | Quality |
| `operational_practicality` | 1.5 | Tier 3 | Quality |
| `comprehensiveness` | 1.2 | Tier 4 | Fast |
| `threat_intel_integration` | 1.2 | Tier 4 | Fast |
| `section_extractor` | N/A | Utility | Quality |

### Example Configuration

See `model_config.json.example` for a complete example:

```json
{
  "version": "1",
  "providers": {
    "anthropic-main": {
      "type": "anthropic",
      "config": {
        "api_key": "${ANTHROPIC_API_KEY}"
      }
    }
  },
  "defaults": {
    "provider": "anthropic-main",
    "model": "claude-sonnet-4-20250514"
  },
  "groups": {
    "tier1-critical": {
      "match": ["technical_accuracy", "query_efficiency", 
                "organization_progression", "template_conformance"],
      "model": "claude-opus-4-1-20250805",
      "comment": "Tier 1 metrics (weight 2.5) - use highest quality model"
    },
    "tier2-quality": {
      "match": ["hypothesis_alignment", "actionability_clarity", 
                "environmental_integration"],
      "model": "claude-sonnet-4-20250514",
      "comment": "Tier 2 metrics (weight 2.0) - use quality model"
    },
    "tier3-standard": {
      "match": ["operational_practicality"],
      "model": "claude-sonnet-4-20250514",
      "comment": "Tier 3 metrics (weight 1.5) - use quality model"
    },
    "tier4-fast": {
      "match": ["comprehensiveness", "threat_intel_integration"],
      "model": "claude-3-5-haiku-20241022",
      "comment": "Tier 4 metrics (weight 1.2) - use fast model"
    },
    "utility": {
      "match": ["section_extractor"],
      "model": "claude-sonnet-4-20250514",
      "comment": "Section extraction utility"
    }
  }
}
```

## Evaluation Criteria Details

### 1. Technical Accuracy (Weight: 2.5) - Tier 1

**What it evaluates:** Correctness of SPL syntax, field names, index references, and technical claims.

**Scoring rubric:**
- **90-100**: All SPL queries syntactically correct, valid field names, proper index references
- **70-89**: Mostly correct with minor syntax issues or field name variations
- **50-69**: Several syntax errors or incorrect field references
- **0-49**: Significant technical errors that would prevent queries from running

**Example feedback:**
- ‚úÖ "All 5 SPL queries validated: correct syntax, valid field names, proper index references"
- ‚ö†Ô∏è "Minor issue: 'process_name' should be 'ProcessName' in Sysmon events"
- ‚ùå "Multiple syntax errors: missing pipes, invalid field names, wrong index references"

### 2. Query Efficiency (Weight: 2.5) - Tier 1

**What it evaluates:** Use of tstats, efficient filtering, proper scoping, and performance optimization.

**Scoring rubric:**
- **90-100**: Extensive use of tstats, early filtering, proper time bounds, optimized searches
- **70-89**: Good efficiency practices, some tstats usage, reasonable filtering
- **50-69**: Basic queries without optimization, could be more efficient
- **0-49**: Inefficient queries that would cause performance issues

**Example feedback:**
- ‚úÖ "Excellent efficiency: 4 of 5 queries use tstats, early filtering, proper time scoping"
- ‚ö†Ô∏è "Good queries but could benefit from tstats on high-volume searches"
- ‚ùå "Inefficient: no tstats usage, late filtering, unbounded searches"

### 3. Organization & Progression (Weight: 2.5) - Tier 1

**What it evaluates:** Logical flow of steps, clear dependencies, and progressive refinement.

**Scoring rubric:**
- **90-100**: Clear logical progression, well-defined dependencies, builds from broad to specific
- **70-89**: Good organization with minor flow issues
- **50-69**: Steps present but progression unclear or illogical
- **0-49**: Disorganized or random step ordering

**Example feedback:**
- ‚úÖ "Excellent progression: starts broad (all LSASS access), narrows to suspicious patterns, ends with validation"
- ‚ö†Ô∏è "Good steps but Step 3 should come before Step 2 for logical flow"
- ‚ùå "Steps are disorganized and don't build on each other"

### 4. Template Conformance (Weight: 2.5) - Tier 1

**What it evaluates:** Presence of all required sections with proper formatting.

**Required sections:**
- Hypothesis
- Recommended Time Frame
- ABLE Table
- Data
- Hunt Procedure

**Scoring rubric:**
- **90-100**: All required sections present with proper markdown formatting
- **70-89**: All sections present, minor formatting issues
- **50-69**: Missing 1-2 sections or poor formatting
- **0-49**: Missing multiple required sections

**Example feedback:**
- ‚úÖ "Perfect template conformance: all 5 required sections present with proper formatting"
- ‚ö†Ô∏è "All sections present but ABLE Table formatting could be improved"
- ‚ùå "Missing 'Recommended Time Frame' and 'ABLE Table' sections"

### 5. Hypothesis Alignment (Weight: 2.0) - Tier 2

**What it evaluates:** How well the hunt plan addresses the original hypothesis.

**Scoring rubric:**
- **90-100**: Hunt plan directly and comprehensively addresses all aspects of hypothesis
- **70-89**: Good coverage of hypothesis with minor gaps
- **50-69**: Partially addresses hypothesis, missing some aspects
- **0-49**: Doesn't align with hypothesis or addresses wrong topic

**Example feedback:**
- ‚úÖ "Excellent alignment: all aspects of credential dumping hypothesis covered in hunt steps"
- ‚ö†Ô∏è "Good coverage but doesn't address the 'memory injection' aspect mentioned in hypothesis"
- ‚ùå "Hunt plan focuses on different technique than hypothesis describes"

### 6. Actionability & Clarity (Weight: 2.0) - Tier 2

**What it evaluates:** Clarity of instructions, executable queries, and clear success criteria.

**Scoring rubric:**
- **90-100**: Crystal clear instructions, all queries ready to run, explicit thresholds and criteria
- **70-89**: Clear instructions with minor ambiguities
- **50-69**: Instructions present but unclear or incomplete
- **0-49**: Vague instructions that can't be followed

**Example feedback:**
- ‚úÖ "Highly actionable: step-by-step instructions, runnable queries, clear thresholds (>5 events = investigate)"
- ‚ö†Ô∏è "Good instructions but success criteria could be more specific"
- ‚ùå "Too vague: 'look for suspicious activity' without defining what that means"

### 7. Environmental Integration (Weight: 2.0) - Tier 2

**What it evaluates:** Consideration of local environment, exclusions, and context.

**Scoring rubric:**
- **90-100**: Includes environment-specific exclusions, allowlists, baselines, considers local context, mentions known-good patterns
- **70-89**: Some environmental considerations
- **50-69**: Minimal environmental context
- **0-49**: No environmental considerations, would generate excessive false positives

**Example feedback:**
- ‚úÖ "Excellent integration: excludes known admin tools via allowlist, references baseline behavior, considers backup processes, mentions local naming conventions"
- ‚ö†Ô∏è "Good but could include more exclusions, allowlists, or baseline references for common legitimate tools"
- ‚ùå "No exclusions, allowlists, or environmental context, would generate many false positives"

### 8. Operational Practicality (Weight: 1.5) - Tier 3

**What it evaluates:** Realistic timeframes, data volume considerations, and resource requirements.

**Scoring rubric:**
- **90-100**: Realistic timeframes, considers data volume, practical for actual execution
- **70-89**: Generally practical with minor concerns
- **50-69**: Some impractical aspects (too long timeframe, excessive data)
- **0-49**: Completely impractical (years of data, impossible queries)

**Example feedback:**
- ‚úÖ "Highly practical: 30-day timeframe, focused queries, reasonable data volumes"
- ‚ö†Ô∏è "90-day timeframe might be too long for high-volume environments"
- ‚ùå "Impractical: 1-year timeframe on unindexed data would take days to run"

### 9. Comprehensiveness (Weight: 1.2) - Tier 4

**What it evaluates:** ABLE framework utilization and data source coverage.

**Scoring rubric:**
- **90-100**: Complete ABLE table, multiple relevant data sources, thorough coverage
- **70-89**: Good ABLE coverage, adequate data sources
- **50-69**: Basic ABLE table, limited data sources
- **0-49**: Incomplete ABLE or minimal data sources

**Example feedback:**
- ‚úÖ "Comprehensive: detailed ABLE table, 5 data sources (Sysmon, Security, EDR, Network, DNS)"
- ‚ö†Ô∏è "Good but could include additional data sources like EDR telemetry"
- ‚ùå "Minimal: basic ABLE table, only 1 data source"

### 10. Threat Intel Integration (Weight: 1.2) - Tier 4

**What it evaluates:** Integration of threat actor TTPs and technique references.

**Scoring rubric:**
- **90-100**: Specific threat actors, TTPs, MITRE ATT&CK references, recent campaigns
- **70-89**: Some threat intel references
- **50-69**: Generic threat intel mentions
- **0-49**: No threat intelligence context

**Example feedback:**
- ‚úÖ "Excellent threat intel: references APT29, Mimikatz usage, MITRE T1003.001, recent campaigns"
- ‚ö†Ô∏è "Mentions MITRE techniques but could include specific threat actor examples"
- ‚ùå "No threat intelligence context or actor references"

## Troubleshooting

### "Required section not found"

**Solution:** Ensure your hunt plan has all required sections:
- Hypothesis
- Recommended Time Frame
- ABLE Table
- Data
- Hunt Procedure

Use proper markdown headers (## Section Name).

### "SPL syntax errors detected"

**Solution:** Validate your SPL queries:
- Check pipe syntax
- Verify field names match your data sources
- Test queries in Splunk before including in plan

### Low scores on query_efficiency

**Solution:** Optimize your queries:
- Use `tstats` for high-volume searches
- Add early filtering (time bounds, index restrictions)
- Use indexed fields when possible

### "model_config.json not found"

**Solution:** Create a `model_config.json` file in your working directory or specify path with `-c`

## Advanced Usage

### Batch Evaluation

Evaluate multiple plans sequentially:

```bash
for plan in plans/*.md; do
  python hunt_plan_evaluator.py "$plan" --output "results/$(basename $plan .md).json"
done
```

### A/B Testing Configurations

Compare plans generated by different configurations:

```bash
# Generate with config A
research-assistant -t "credential dumping" -s --config config-a.json
mv output.md plan-config-a.md

# Generate with config B
research-assistant -t "credential dumping" -s --config config-b.json
mv output.md plan-config-b.md

# Compare
python hunt_plan_evaluator.py plan-config-a.md plan-config-b.md
```

### Integration with CI/CD

```bash
#!/bin/bash
# Validate hunt plan quality in CI pipeline

python hunt_plan_evaluator.py plan.md --output results.json -q

# Extract score and fail if below threshold
score=$(jq '.evaluations[0].total_score' results.json)
if (( $(echo "$score < 80" | bc -l) )); then
  echo "Hunt plan quality too low: $score"
  exit 1
fi
```

## See Also

- [Main Evaluations README](../README.md) - Common configuration and setup
- [Hypothesis Evaluator](../hypothesis-eval/README.md) - Evaluate hypotheses
- [Research Evaluator](../research-agent-team-eval/README.md) - Evaluate research reports
- `model_config.json.example` - Example configuration file
